{
    "metadata": {
        "language_info": {
            "name": "python", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "file_extension": ".py", 
            "version": "2.7.11"
        }, 
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20", 
            "language": "python"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "source": "<p>\n<img src=\"https://alanattableau.files.wordpress.com/2014/12/cropped-databender4.png\" align=center>\n<h1 align=center>The Last Databenders</h1>\n<h4 align=center>Sponsored By Gutenberg</h4>\n<img src=\"http://www.gutenberg.org/pics/logo-144x144.png\" align=center>\n</p>", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Remove the unnecessary text from files.\n# Word groups which are identifying beginning and the end of necessary text.\n\n# Word groups to find the beginning of text\nHEADER_MARKERS = [\n\n      '* START OF THE PROJECT GUTENBERG',\n      '* START OF THIS PROJECT GUTENBERG',\n      'This etext was prepared by',\n      'E-text prepared by',\n      'Produced by',\n      'Distributed Proofreading Team',\n      '*END THE SMALL PRINT',\n      '***START OF THE PROJECT GUTENBERG',\n      'This etext was produced by',\n      '* START OF THE COPYRIGHTED',\n      'The Project Gutenberg',\n      'http://gutenberg.spiegel.de/ erreichbar.',\n      'Project Runeberg publishes',\n      'Beginning of this Project Gutenberg',\n      'Project Gutenberg Online Distributed',\n      'Gutenberg Online Distributed',\n      'the Project Gutenberg Online Distributed',\n      'Project Gutenberg TEI',\n      'This eBook was prepared by',\n      'http://gutenberg2000.de erreichbar.',\n      'This Etext was prepared by',\n      'Gutenberg Distributed Proofreaders',\n      'the Project Gutenberg Online Distributed Proofreading Team',\n      '**The Project Gutenberg',\n      '*SMALL PRINT!',\n      'More information about this book is at the top of this file.',\n      'tells you about restrictions in how the file may be used.',\n      'of the etext through OCR.',\n      '**These eBooks Were Prepared By Thousands of Volunteers!**',\n      'SERVICE THAT CHARGES FOR DOWNLOAD',\n      'We need your donations more than ever!',\n      ' * START OF THIS PROJECT GUTENBERG',\n      '**     SMALL PRINT!'\n    \n]\n\n# Word groups to find the end of the text.\nFOOTER_MARKERS = [\n    '* END OF THE PROJECT GUTENBERG',\n    '* END OF THIS PROJECT GUTENBERG',\n    '***END OF THE PROJECT GUTENBERG',\n    'End of the Project Gutenberg',\n    'End of The Project Gutenberg',\n    'by Project Gutenberg',\n    'End of Project Gutenberg',\n    'End of this Project Gutenberg',\n    '        ***END OF THE PROJECT GUTENBERG',\n    '* END OF THE COPYRIGHTED',\n    'End of this is COPYRIGHTED',\n    '*This is a COPYRIGHTED Project Gutenberg Etext, Details Above*',\n    'More information about this book is at the top of this file.',\n    'We need your donations more than ever!',\n    '<<THIS ELECTRONIC VERSION OF',\n    'END OF PROJECT GUTENBERG',\n    ' End of the Project Gutenberg',\n    ' * END OF THIS PROJECT GUTENBERG',\n    \"THE END\"  \n]", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 1, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# Necessary functions to get and transform the data into an easily processable dataset.\n\n# the libs that handle the url stuff\nimport urllib2, string, json  \nfrom bs4 import BeautifulSoup\n\n# Standardization of the input data.\n# Input type is string and represents data.\ndef ParseBook(data):\n    is_header = False # Identifies if necessary text started.\n    is_footer = False # Identifies if necessary text finished.\n    for line in data:\n        for footer in FOOTER_MARKERS:\n            if footer in line.strip(): # Check if the current line includes a footer marker.\n                is_footer = True # If footer marker found, sign the line as the ending point and interrupt the process.\n                break\n        if is_footer:\n            break\n        if is_header: # If headers removed, then proceed.\n            for i in line.split():\n                # STANDARDIZATION PROCESS, word by word\n                word = i.strip().lower().translate(None, string.punctuation)\n                tx.write(word+\" \") # One large dataset txt file on disk, that will be processed in ENGINE.\n\n        for header in HEADER_MARKERS: # Check if the current line includes a header marker.\n            if header in line.strip(): # If footer marker found, sign the line as the beginning point and allow standardization process.\n                is_header = True\n                break\n\n# Take the book from the URL.\n# Inputs are an Integer and a String represents the \u0131ndex of the book and the category of the book respectively.                \ndef FillDict(index, cat):\n    tx.write(cat+\"\\t\") # Mark the book with its category; tab-separeted structure.\n    try:\n        data = urllib2.urlopen(\"http://www.gutenberg.org/files/\"+str(index)+\"/\"+str(index)+\"-0.txt\") # it's a file like object and works just like a file\n        ParseBook(data)\n    except urllib2.HTTPError:\n        try:\n            data = urllib2.urlopen(\"http://www.gutenberg.org/files/\"+str(index)+\"/\"+str(index)+\".txt\") # it's a file like object and works just like a file\n            ParseBook(data)\n        except:\n            pass\n    tx.write(\"\\n\") # Separate each book via new-line operator.\n\n# Create a dataset of books; specified here as only children books\n### Our script also calls this function for random categories to create a training dataset, \n### those codes are not included here.\ndef FindBookIndex():\n    categorise = ['Children']\n    for cat in categorise: \n        book_index = 0\n        while book_index < 100: # The size of the dataset can be specified here.\n            try: # Some categories could include less number of books than expected and raises HTTPError.\n                html_doc = urllib2.urlopen(\"http://www.gutenberg.org/ebooks/search/?start_index=\"+str(book_index)+\"&query=\"+cat+\"+l.english\").read()\n                soup = BeautifulSoup(html_doc, 'html.parser')\n                lis = soup.find_all(\"li\",  { \"class\" : \"booklink\" })\n                for i in range(len(lis)):\n                    index = lis[i].a['href'].split(\"/\")[2]\n                    FillDict(index, cat)\n                book_index += 26\n            except urllib2.HTTPError: # Handling\n                break\n                \ntx = open(\"main_Children.txt\", \"w\") # Our main Training Data File. \n                                    # It is persisten on disk as txt so that we do not need to run this script each time.\nFindBookIndex()\ntx.close()\nprint \"DONE\"", 
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 2, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Children\nDONEE\n"
                }
            ]
        }, 
        {
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 0
}