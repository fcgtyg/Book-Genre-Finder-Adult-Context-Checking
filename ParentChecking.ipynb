{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<p>\n<img src=\"https://alanattableau.files.wordpress.com/2014/12/cropped-databender4.png\" align=center>\n<h1 align=center>The Last Databenders</h1>\n<h4 align=center>Sponsored By Gutenberg</h4>\n<img src=\"http://www.gutenberg.org/pics/logo-144x144.png\" align=center>\n</p>"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 13, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "cell_type": "code", 
            "execution_count": 14, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "cell_type": "code", 
            "execution_count": 15, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# SLUNG WORDS DATASET INPUT FILE\npath_1 = \"swift://TheLastDatabendersCS340.\" + name + \"/en.txt\"\nslang_rdd = sc.textFile(path_1) # Slung word filtering data as RDD.\n\n### Your data file was loaded into a StringIO object and you can process the data.\n### Please read the documentation of pandas to learn more about your possibilities to load your data.\n### pandas documentation: http://pandas.pydata.org/pandas-docs/stable/io.html"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 16, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# UNNECESSARY WORD FILTER INITIALIZATION\n\n# Stopwords which are not meaningful alone in ENGLISH.\nstopwords = ['a', \"a's\", 'able', 'about', 'above', 'according',\"project\",\"gutenberg\",\"ebook\",'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', \"c'mon\", \"c's\", 'came', 'can', \"can't\", 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly',\"the\", 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', \"couldn't\", 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', \"don't\", 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going',\"things\",\"electronic work\",\"electronic\",\"work\",'gone', 'got', 'gotten', 'greetings', 'h', 'had', \"hadn't\", 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he's\", 'hello', 'help', 'hence', 'her', 'here', \"here's\", 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', \"let's\", 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', \"shouldn't\", 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', \"t's\", 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that's\", 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', \"there's\", 'thereafter', 'thereby',\"place\",\"moment\", 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', \"wasn't\", 'way', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'welcome', 'well', 'went', 'were', \"weren't\", 'what', \"what's\", 'whatever', 'when', 'whence', 'whenever', 'where', \"where's\", 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', \"who's\", 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', \"won't\", 'wonder', 'would', 'would', \"wouldn't\", 'x', 'y', 'yes', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero',\"made\",\"time\",\"thing\",\"man\",\"hand\",\"men\",\"word\",\"thought\",\"day\",\"electronic work\",\"found\"]\n\n# Adjectives are not worth for consider in Learning algorithm.\n### Based on our analyses.\nadjectives = ['able', 'bad', 'best',\"back\", 'better', 'big', 'black', 'certain', 'clear', 'early', 'easy', 'free', 'full', 'good', 'great', 'hard', 'high', 'important', 'large', 'late', 'little', 'local', 'long', 'low', 'major', 'new', 'old', 'only', 'other', 'political', 'possible', 'public', 'real', 'recent', 'right', 'small', 'social', 'special', 'strong', 'sure', 'true', 'white', 'whole', 'young']\n\n# Common words for most books, which are casing a normalization on word counts.\n### Based on our analyses.\ncommon_verbs = ['say', 'make', 'go', 'take', 'come', 'see', 'know', 'get', 'give', 'find', 'think', 'tell', 'become', 'show', 'leave', 'feel', 'put', 'bring', 'begin', 'keep', 'hold', 'write', 'stand', 'hear', 'let', 'mean', 'set', 'meet', 'run', 'pay', 'sit', 'speak', 'lie', 'lead', 'read', 'grow', 'lose', 'fall', 'send', 'build', 'draw', 'break', 'spend', 'cut', 'rise', 'drive', 'buy', 'wear', 'choose', 'said', 'made', 'went', 'took', 'came', 'saw', 'knew', 'got', 'gavefound', 'thought', 'told', 'became', 'showed', 'left', 'felt', 'put', 'brought', 'began', 'kept', 'held', 'wrote', 'stood', 'heard', 'let', 'meant', 'set', 'met', 'ran', 'paid', 'sat', 'spoke', 'lay', 'led', 'read', 'grew', 'lost', 'fell', 'sent', 'built', 'drew', 'broke', 'spent', 'cut', 'rose', 'drove', 'bought', 'wore', 'chose', 'gone', 'taken', 'seen', 'known', 'given', 'shown', 'written', 'spoken', 'lain', 'grown', 'fallen', 'drawn', 'broken', 'risen', 'driven', 'worn', 'chosen', 'wanted', 'used', 'worked', 'called', 'tried', 'asked', 'needed', 'seemed', 'helped', 'played', 'moved', 'lived', 'believed', 'happened', 'included', 'continued', 'changed', 'watched', 'followed', 'stopped', 'created', 'opened', 'walked', 'offered', 'remembered', 'appeared', 'served', 'died', 'stayed', 'reached', 'killed', 'raised', 'passed', 'decided', 'returned', 'explained', 'hoped', 'carried', 'received', 'agreed', 'covered', 'caused', 'closed', 'increased', 'reduced', 'noted', 'entered', 'shared', 'saved', 'protected', 'occurred', 'accepted', 'determined', 'prepared', 'argued', 'recognized', 'indicated', 'arrived', 'answered', 'compared', 'acted', 'studied', 'removed', 'sounded', 'formed', 'visited', 'avoided', 'imagined', 'finished', 'responded', 'maintained', 'contained', 'applied', 'treated', 'affected', 'worried', 'mentioned', 'improved', 'signed', 'existed', 'noticed', 'travelled', 'prevented', 'admitted', 'suffered', 'published', 'counted', 'achieved', 'announced', 'touched', 'attended', 'defined', 'introduced']\n\nfinal_stopwords = stopwords + adjectives + common_verbs"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 17, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Needed libraries for Machine Learning.\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\nfrom pyspark.sql import Row\n\n# RDD of the training set that we constructed at DATASET Notebook.\nrdd = sc.textFile(\"main_Categories.txt\")\n\n# Data training algorithms.\n### Input: A string represents genre type we want to train dataset on.\ndef TrainData(genre):\n    interim = rdd.map(lambda line: (1.0 if line.split(\"\\t\")[0] == genre else 0.0, line.split(\"\\t\")[1]))\n    interim = interim.map(lambda t: Row(label=t[0], text=t[1]))\n    \n    training = sqlContext.createDataFrame(interim).cache()\n    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"word\") # Splitting words.\n    stopwordremover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\", stopWords=final_stopwords) # Removing stopwords.\n    hashingTF = HashingTF(inputCol=stopwordremover.getOutputCol(), outputCol=\"features\") # Feature vector creation.\n    lr = LogisticRegression(maxIter=10, regParam=0.01)\n    \n    pipeline = Pipeline(stages=[tokenizer, stopwordremover, hashingTF, lr])\n\n    # Fit the pipeline to training documents.\n    model = pipeline.fit(training)\n    return model"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 18, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "model = TrainData(\"Children\") # Model for CHILDREN genre."
        }, 
        {
            "cell_type": "code", 
            "execution_count": 19, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "############## TEST SECTION ####################"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 20, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Our test books. \ntestlist = [\"test_1_F.txt\",\"test_2_F.txt\",\"test_3_F.txt\",\"test_4_F.txt\",\"test_5_F.txt\",\"test_6_F.txt\",\"test_1_N.txt\",\"test_1_T.txt\",\"test_2_T.txt\",\"test_3_T.txt\",\"test_4_T.txt\",\"test_5_T.txt\",\"test_6_T.txt\"]"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 35, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# Testing function. \n### Input is a string which is the file name. \n### FILE MUST BE UPLOADED ON DATA STORAGE.\ndef test(book):  \n    print book\n    # Open the test book TXT and assign it to a String Variable:\n    test = get_object_storage_file_with_credentials_9178c0c603054adab28f2ffa6291055a('TheLastDatabendersCS340', book).read()\n    \n    testDF = sqlContext.createDataFrame([Row(text=test)]) # Convert test string to DataFrame.\n    result = model.transform(testDF) # Insert the DataFrame into model we created above.\n    \n    prediction = result.select(\"prediction\").rdd.map(tuple).collect()[0][0]\n    \n    # Create RDDs for slang word filtering. slang_rdd ALREADY CREATED ABOVE.\n    test_rdd = sc.parallelize(test.replace(\"\\r\",\" \").replace(\"\\n\",\" \").split(\" \")).map(lambda line: line.lower()).filter(lambda line: line != '')\n    \n    slang_match = len(slang_rdd.intersection(test_rdd).collect()) #Number of slang words in the book.\n    if slang_match == 0: # If no slang words found; proceed.\n        if prediction == 1.0: \n            print \"It is appropriate for children.\"\n        else: \n            print \"It is inappropriate for children. It contains sexuality and/or violence.\"\n    elif slang_match < 7: # If there are acceptable number of slang words; proceed.\n        print \"It contains a few slang words.\"    \n        if prediction == 1.0:\n            print \"It is recommended to be read under parental supervision.\"    \n        else:\n            print \"It is inappropriate for children. It contains sexuality and/or violence.\"        \n    else: # If slang words are much more; proceed.\n        print \"It is inappropriate for children. It may contain intense violence, sexual content and/or bad language.\"\n    "
        }, 
        {
            "cell_type": "code", 
            "execution_count": 34, 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "test_1_F.txt\nIt is inappropriate for children. It may contain intense violence, sexual content and/or strong language.\ntest_2_F.txt\nIt contains a few slang words.\nIt is inappropriate for children. It contains sexuality and/or violence.\ntest_3_F.txt\nIt is inappropriate for children. It may contain intense violence, sexual content and/or strong language.\ntest_4_F.txt\nIt contains a few slang words.\nIt is recommended to be read under parental supervision.\ntest_5_F.txt\nIt is inappropriate for children. It may contain intense violence, sexual content and/or strong language.\ntest_6_F.txt\nIt contains a few slang words.\nIt is recommended to be read under parental supervision.\ntest_1_N.txt\nIt contains a few slang words.\nIt is recommended to be read under parental supervision.\ntest_1_T.txt\nIt is appropriate for children.\ntest_2_T.txt\nIt is appropriate for children.\ntest_3_T.txt\nIt is appropriate for children.\ntest_4_T.txt\nIt is appropriate for children.\ntest_5_T.txt\nIt is appropriate for children.\ntest_6_T.txt\nIt contains a few slang words.\nIt is recommended to be read under parental supervision.\n"
                }
            ], 
            "source": "for book in testlist:\n    test(book)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "language": "python", 
            "name": "python2-spark20"
        }, 
        "language_info": {
            "name": "python", 
            "pygments_lexer": "ipython2", 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat_minor": 0
}